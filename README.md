## Seq2seq Neural Grammar Correction

### Goal

The goal of this project is to experiment with elmo embedding and to see if there's an improvement when we change the embedding to elmo for sequence-to-sequence translation for grammar correction. 

### Dataset 

The dataset are from CoNLL-2013 and CoNLL-2014 Shared Task for grammar correction. They have original sentence and corrected sentence with position of error in the sentence and error type. CoNLL-2013 has 5 types of errors while CoNLL-2014 has 28 types of errors. 

### Virtualenv

You need two virtualenvs named allennlp and torch. allennlp is for elmo embedding, and torch is for machine translation. GPU is required for generating new elmo embeddings and Python3 is used.

[elmo] https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md 
* allennlp

        pip install allennlp

[NMT] https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
* torch

        pip install matplotlib
        pip install torch==0.4.0 
        pip install torchvision

### Script

```
bash grammar.sh \
    --emb <embedding_path> \
    --encoder <encoder_path> \
    --decoder <decoder_path> \
    --sentences <sentences_path> 
```

Below is an example of training using `train_small.elmo` embedding. `train_small.elmo` is already in `CoNLL_data` folder. `train.elmo` and `train_baseline.elmo` can be generated by `elmo.py CoNLL_data/train.txt CoNLL_data/train.elmo` and `elmo.py CoNLL_data/baseline_train.txt CoNLL_data/train_baseline.elmo` or using `bash grammar.sh` with the correct parameters. 

```
bash grammar.sh \
   --emb data/CoNLL_data/train_small.elmo \
   --encoder data/models/with_error_tag.encoder \
   --decoder data/models/with_error_tag.decoder \
   --sentences data/CoNLL_data/train.txt
```
